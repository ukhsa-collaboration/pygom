

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Gradient estimation under square loss &#8212; PyGOM documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/gradient';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Example: Fitz Hugh" href="fh.html" />
    <link rel="prev" title="Solving Boundary Value Problems" href="bvpSimple.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo_pygom.jpg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo_pygom.jpg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to the documentation for PyGOM
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">User documentation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../markdown.html">Markdown Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="../md/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir.html">Motivating Example: SIR Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="transition.html">Transition Object</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic.html">Stochastic representation of ODEs</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../md/unrollOde.html">Convert ODE into transitions</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="unroll/unrollSimple.html">Simple Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="unroll/unrollBD.html">ODE With Birth and Death Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="unroll/unrollHard.html">Hard Problem</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="epi.html">Simple Epidemic Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="epijson.html">Reading and using EpiJSON data</a></li>
<li class="toctree-l1"><a class="reference internal" href="bvpSimple.html">Solving Boundary Value Problems</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Gradient estimation under square loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="fh.html">Example: Fitz Hugh</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ukhsa-collaboration/pygom" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ukhsa-collaboration/pygom/issues/new?title=Issue%20on%20page%20%2Fnotebooks/gradient.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/gradient.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gradient estimation under square loss</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-setup">Model setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-sensitivity">Forward sensitivity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient">Gradient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobian">Jacobian</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjoint">Adjoint</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-comparison">Time Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian">Hessian</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gradient-estimation-under-square-loss">
<h1>Gradient estimation under square loss<a class="headerlink" href="#gradient-estimation-under-square-loss" title="Permalink to this heading">#</a></h1>
<p>Assuming that we have a set of <span class="math notranslate nohighlight">\(N\)</span> observations <span class="math notranslate nohighlight">\(y_{i}\)</span> at specific time
points <span class="math notranslate nohighlight">\(t_{i}\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots,N\)</span>, we may wish to test out a set of ode
to see whether it fits to the data. The most natural way to test such
<em>fit</em> is to minimize the sum of squares between our observations <span class="math notranslate nohighlight">\(y\)</span> and
see whether the resulting solution of the ode and the estimationed
parameters makes sense.</p>
<p>We assume that this estimation process will be tackled through a
non-linear optimization point of view. However, it should be noted that
such estimates can also be performed via MCMC or from a global
optimization perspective. A key element in non-linear optimization is
the gradient, which is the focus of this page.</p>
<p>Multiple ways of obtaining the gradient have been implemented. All of
them serve a certain purpose and may not be a viable/appropriate options
depending on the type of ode. More generally, let <span class="math notranslate nohighlight">\(d,p\)</span> be the number of
states and paramters respectively. Then finite difference methods have a
run order of <span class="math notranslate nohighlight">\(O(p+1)\)</span> of the original ode, forward sensitivity require
an integration of an ode of size <span class="math notranslate nohighlight">\((d+1)p\)</span> rather than <span class="math notranslate nohighlight">\(d\)</span>. The adjoint
method require two run of size <span class="math notranslate nohighlight">\(d\)</span> in principle, but actual run time is
dependent on the number of observations.</p>
<p>For the details of the classes and methods, please refer to <code class="docutils literal notranslate"><span class="pre">mod</span></code>.</p>
<section id="notation">
<h2>Notation<a class="headerlink" href="#notation" title="Permalink to this heading">#</a></h2>
<p>We introduce the notations that will be used in the rest of the page,
some of which may be slightly unconventional but necessary due to the
complexity of the problem. Let <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{d}\)</span> and
<span class="math notranslate nohighlight">\(\theta \in \mathbb{R}^{p}\)</span> be the states and parameters respectively.
The term <em>state</em> or <em>simulation</em> are used interchangeably, even though
strictly speaking a state is <span class="math notranslate nohighlight">\(x\)</span> whereas <span class="math notranslate nohighlight">\(x(t)\)</span> is the simulation. An
ode is defined as</p>
<div class="math notranslate nohighlight">
\[f(x,\theta) = \dot{x} = \frac{\partial x}{\partial t}\]</div>
<p>and usually comes with a set of initial conditions <span class="math notranslate nohighlight">\((x_0,t_0)\)</span> where
<span class="math notranslate nohighlight">\(t_0 \le t_{i} \forall i\)</span>. Let <span class="math notranslate nohighlight">\(g(x,\theta)\)</span> be a function that maps the
set of states to the observations,
<span class="math notranslate nohighlight">\(g : \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}\)</span>. For compartmental
problems, which is our focus, <span class="math notranslate nohighlight">\(\nabla_{\theta}g(x,\theta)\)</span> is usually
zero and <span class="math notranslate nohighlight">\(\nabla_{x}g(x,\theta)\)</span> is an identity function for some or all
of the states <span class="math notranslate nohighlight">\(x\)</span>. Denote <span class="math notranslate nohighlight">\(l(x_{0},\theta,x)\)</span> as our cost function
<span class="math notranslate nohighlight">\(l : \mathbb{R}^{m} \rightarrow \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(L(x_{0},\theta,x)\)</span> be
the sum of <span class="math notranslate nohighlight">\(l(\cdot)\)</span>. Both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x_{0}\)</span> are usually dropped for
simplicity. We will be dealing exclusively with square loss here, which
means that</p>
<div class="math notranslate nohighlight">
\[L(\theta) = \sum_{i=1}^{N} \left\| y_{i} - g(x(t_{i})) \right\|^{2} = \mathbf{e}^{\top} \mathbf{e}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> is the residual vector, with elements</p>
<div class="math notranslate nohighlight">
\[e_{i} = y_{i} - x(t_{i}).\]</div>
</section>
<section id="model-setup">
<h2>Model setup<a class="headerlink" href="#model-setup" title="Permalink to this heading">#</a></h2>
<p>Again, we demonstrate the functionalities of our classes using an SIR
model.</p>
<p>In [1]: from pygom import SquareLoss, common_models</p>
<p>In [2]: import copy,time,numpy</p>
<p>In [2]: ode = common_models.SIR()</p>
<p>In [3]: paramEval = [(‘beta’,0.5), (‘gamma’,1.0/3.0) ]</p>
<p>In [7]: # the initial state, normalized to zero one</p>
<p>In [8]: x0 = [1., 1.27e-6, 0.]</p>
<p>In [5]: # initial time</p>
<p>In [6]: t0 = 0</p>
<p>In [5]: ode.parameters = paramEval</p>
<p>In [6]: ode.initial_values = (x0, t0)</p>
<p>In [9]: # set the time sequence that we would like to observe</p>
<p>In [10]: t = numpy.linspace(1, 150, 100)</p>
<p>In [11]: numStep = len(t)</p>
<p>In [11]: solution = ode.integrate(t)</p>
<p>In [12]: y = solution[1::,2].copy()</p>
<p>In [13]: y += numpy.random.normal(0, 0.1, y.shape)</p>
<p>Now we have set up the model along with some observations, obtaining the
gradient only requires the end user to put the appropriate information
it into the class <code class="docutils literal notranslate"><span class="pre">SquareLoss</span></code>. Given the initial guess <span class="math notranslate nohighlight">\(\theta\)</span></p>
<p>In [210]: theta = [0.2, 0.2]</p>
<p>We initialize the <code class="docutils literal notranslate"><span class="pre">SquareLoss</span></code> simply as</p>
<p>In [20]: objSIR = SquareLoss(theta, ode, x0, t0, t, y, ‘R’)</p>
<p>where the we also have to specify the state our observations are from.
Now, we demonstrate the different methods in obtaining the gradient and
mathematics behind it.</p>
</section>
<section id="forward-sensitivity">
<h2>Forward sensitivity<a class="headerlink" href="#forward-sensitivity" title="Permalink to this heading">#</a></h2>
<p>The forward sensitivity equations are derived by differentiating the
states implicitly, which yields</p>
<div class="math notranslate nohighlight">
\[\frac{d\dot{x}}{d\theta} = \frac{\partial f}{\partial x}\frac{dx}{d\theta} + \frac{\partial f}{\partial \theta}.\]</div>
<p>So finding the sensitivies <span class="math notranslate nohighlight">\(\frac{dx}{d\theta}\)</span> simply require another
integration of a <span class="math notranslate nohighlight">\(p\)</span> coupled ode of <span class="math notranslate nohighlight">\(d\)</span> dimension, each with the same
Jacobian as the original ode. This integration is performed along with
the original ode because of possible non-linearity.</p>
<p>A direct call to the method <code class="docutils literal notranslate"><span class="pre">sensitivity</span> <span class="pre">&lt;pygom.SquareLoss.sensitivity&gt;</span></code>
computed the gradient</p>
<p>In [33]: gradSens = objSIR.sensitivity()</p>
<p>whereas <code class="docutils literal notranslate"><span class="pre">.jac</span></code> will allow the end user to obtain the Jacobian (of the
objective function) and the residuals, the information required to get
the gradient as we see next.</p>
<p>In [33]: objJac, output = objSIR.jac(full_output=True)</p>
</section>
<section id="gradient">
<h2>Gradient<a class="headerlink" href="#gradient" title="Permalink to this heading">#</a></h2>
<p>Just the sensitivities alone are not enough to obtain the gradient, but
we are <span class="math notranslate nohighlight">\(90\%\)</span> there. Differentiating the loss function</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{dL}{d\theta} &amp;= \nabla_{\theta} \sum_{i=1}^{N}\frac{dl}{dg} \\
                   &amp;= \sum_{i=1}^{N} \frac{\partial l}{\partial x}\frac{dx}{d\theta} + \frac{\partial l}{\partial \theta} \\
                   &amp;= \sum_{i=1}^{N} \frac{\partial l}{\partial g}\frac{\partial g}{\partial x}\frac{dx}{d\theta} + \frac{\partial l}{\partial g}\frac{\partial g}{\partial \theta}
\end{aligned}\end{split}\]</div>
<p>via chain rule. When <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial \theta} = 0\)</span>, the total
gradient simplifies to</p>
<div class="math notranslate nohighlight">
\[\frac{dL}{d\theta} = \sum_{i=1}^{N} \frac{\partial l}{\partial g}\frac{\partial g}{\partial x}\frac{dx}{d\theta}\]</div>
<p>Obviously, the time indicies are dropped above but all the terms above
are evaluated only at the observed time points. More concretely, this
means that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial l(x(j),\theta)}{\partial g} = \left\{ \begin{array}{ll} -2(y_{i} - x(j)) &amp; , \; j = t_{i} \\ 0 &amp; \; \text{otherwise} \end{array} \right.
\end{aligned}\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\(g(\cdot)\)</span> is an identity function (which is assumed to be the case
in <code class="docutils literal notranslate"><span class="pre">SquareLoss</span></code>)</p>
<div class="math notranslate nohighlight">
\[\frac{\partial g(x(t_{i}),\theta)}{\partial x} = I_{d}\]</div>
<p>then the gradient simplifies even further as it is simply</p>
<div class="math notranslate nohighlight">
\[\frac{dL}{d\theta} = -2\mathbf{e}^{\top}\mathbf{S}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> is the vector of residuals and
<span class="math notranslate nohighlight">\(\mathbf{S} = \left[\mathbf{s}_{1},\mathbf{s}_{2},\ldots,\mathbf{s}_{n}\right]\)</span>
with elements</p>
<div class="math notranslate nohighlight">
\[\mathbf{s}_{i} = \frac{dx}{d\theta}(t_{i}),\]</div>
<p>the solution of the forward sensitivies at time <span class="math notranslate nohighlight">\(t_{i}\)</span>, obtained from
solving the coupled ode as mentioned previously.</p>
</section>
<section id="jacobian">
<h2>Jacobian<a class="headerlink" href="#jacobian" title="Permalink to this heading">#</a></h2>
<p>Now note how the gradient simplifies to <span class="math notranslate nohighlight">\(-2\mathbf{e}^{\top}\mathbf{S}\)</span>.
Recall that a standard result in non-linear programming states that the
gradient of a sum of sqaures objective function <span class="math notranslate nohighlight">\(L(\theta,y,x)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta} L(\theta,y,x) = -2(\mathbf{J}^{T} \left[\mathbf{y} - \mathbf{f}(x,\boldsymbol{\theta}) \right] )^{\top}\]</div>
<p>with <span class="math notranslate nohighlight">\(f(x,\theta)\)</span> our non-linear function and <span class="math notranslate nohighlight">\(J\)</span> our Jacobian with
elements</p>
<div class="math notranslate nohighlight">
\[J_{i} = \frac{\partial f(x_{i},\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}.\]</div>
<p>This is exactly what we have seen previously, substituting in reveals
that <span class="math notranslate nohighlight">\(J = \mathbf{S}\)</span>. Hence, the Jacobian is (a necessary)by product
when we wish to obtain the gradient. In fact, this is exactly how we
proceed in <code class="docutils literal notranslate"><span class="pre">sensitivity</span> <span class="pre">&lt;pygom.SquareLoss.sensitivity&gt;</span></code> where it makes
an internal call to <code class="docutils literal notranslate"><span class="pre">jac</span> <span class="pre">&lt;pygom.SqaureLoss.jac&gt;</span></code> to obtain the Jacobian
first. This allows the end user to have more options when choosing which
type of algorithms to use, i.e. Gauss-Newton or Levenberg-Marquardt.</p>
<p>To check that the output is in fact the same</p>
<p>In [1]: objJac.transpose().dot(-2*output[‘resid’]) - gradSens</p>
</section>
<section id="adjoint">
<h2>Adjoint<a class="headerlink" href="#adjoint" title="Permalink to this heading">#</a></h2>
<p>When the number of parameters increases, the number of sensitivies also
increases. The time required scales directly with the number of
parameters. We describe another method which does not depend on the
number of parameters, but rather, the number of states and observations.</p>
<p>The full derivations will not be shown here, but we aim to provide
enough information to work out the steps performed in the our code. Let
write our optimization problem as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
min_{\theta} \quad &amp; \int_{t_{0}}^{T} l(x_{0},\theta,x(t)) dt \\
s.t. \quad &amp; \dot{x} = f(x,\theta)
\end{aligned}\end{split}\]</div>
<p>which is identical to the original problem but in a continuous setting.
Now write the constrained problem in the Lagrangian form</p>
<div class="math notranslate nohighlight">
\[min_{\theta} \; L(\theta) + \int_{t_{0}}^{T} \lambda^{\top}(\dot{x} - f(x,\theta))\]</div>
<p>with Lagrangian multiplier <span class="math notranslate nohighlight">\(\lambda \ge 0\)</span>. After some algebraic
manipulation, it can be shown that the total derivative of the
Lagrangian function is</p>
<div class="math notranslate nohighlight">
\[\frac{dL}{d\theta} = \int_{t_{0}}^{T} \left(\frac{\partial l}{\partial \theta} - \lambda^{\top}\frac{\partial f}{\partial \theta} \right) dt.\]</div>
<p>Using previously defined loss functions (the identity), the first term
is zero and evaluating <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial \theta}\)</span> is trivial.
What remains is the calculation of <span class="math notranslate nohighlight">\(\lambda(t)\)</span> for
<span class="math notranslate nohighlight">\(t \in \left[t_{0},T\right]\)</span>.</p>
<p>Although this still seem to be ill-posed problem when Looking at the
Lagrangian function, one can actually obtain the <em>adjoint equation</em>,
after certain assumptions and</p>
<div class="math notranslate nohighlight">
\[\frac{d\lambda^{\top}}{dt} = \frac{\partial l}{\partial x} - \lambda^{\top}\frac{\partial f}{\partial \theta}.\]</div>
<p>which is again an integration. An unfortunate situation arise here for
non-linear systems because we use the minus Jacobian in the adjoint
equation. So if the eigenvalues of the Jacobian indicate that our
original ode is stable, such as -1, the minus eigenvalues (now 1)
implies that the adjoint equation is not stable. Therefore, one must
integrate backward in time to solve the adjoint equation and it cannot
be solved simultaneously as the ode, unlike the forward sensitivity
equations.</p>
<p>Given a non-linearity ode, we must store information about the states
between <span class="math notranslate nohighlight">\(t_{0}\)</span> and <span class="math notranslate nohighlight">\(T\)</span> in order to perform the integration. There are
two options, both require storing many evaluated <span class="math notranslate nohighlight">\(x(j)\)</span> within the
interval <span class="math notranslate nohighlight">\(\left[t_{0},T\right]\)</span>. Unfortunately, only one is available;
interpolation over all states and integrate using the interpolating
functions. The alternative of using observed <span class="math notranslate nohighlight">\(x(j)'s\)</span> at fixed points is
not competitive because we are unable to use fortran routines for the
integration</p>
<p>The method of choice here to perform the adjoint calcuation is to run a
forward integration, then perform an interpolation using splines with
explicit knots at the observed time points.</p>
<p>In [326]: odeSIRAdjoint, outputAdjoint =
objSIR.adjoint(full_output=True)</p>
<p>This is because evaluating the Jacobian may be expensive and Runge-kutta
method suffers as the complexity increases. In non-linear model such as
those found in epidemiology, each element of the Jacobian may be the
result of a complicated equation where linear step method will shine as
it makes as little function evaluation as possible. Note that
derivations in the literature, the initial condition when evaluating the
adjoint equation is <span class="math notranslate nohighlight">\(\lambda(T)=0\)</span>. But in our code we used
<span class="math notranslate nohighlight">\(\lambda(T) = -2(y(T)-x(T))\)</span>. Recall that we have observation <span class="math notranslate nohighlight">\(y(T)\)</span> and
simulation <span class="math notranslate nohighlight">\(x(T)\)</span>, so that the adjoint equation evaluated at time <span class="math notranslate nohighlight">\(T\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{\partial \lambda^{\top}}{\partial t} \Big|_{T} = -2(y-f(x,\theta))\Big|_{T}  - \lambda(T)\frac{\partial f}{\partial \theta}\Big|_{T}\]</div>
<p>with the second term equal to zero. Integration under step size <span class="math notranslate nohighlight">\(h\)</span>
implies that
<span class="math notranslate nohighlight">\(\lambda(T) \approx \lim_{h \to 0} \lambda(T-h) = -2(y(T)-x(T))\)</span>.</p>
</section>
<section id="time-comparison">
<h2>Time Comparison<a class="headerlink" href="#time-comparison" title="Permalink to this heading">#</a></h2>
<p>A simple time comparison between the different methods reveals that the
forward sensitivity method dominates the others by a wide margin. It
will be tempting to conclude that it is the best and should be the
default at all times but that is not true, due to the complexity of each
method mentioned previously. We leave it to the end user to find out the
best method for their specific problem.</p>
<p>In [319]: %timeit gradSens = objSIR.sensitivity()</p>
<p>In [326]: %timeit odeSIRAdjoint,outputAdjoint =
objSIR.adjoint(full_output=True)</p>
</section>
<section id="hessian">
<h2>Hessian<a class="headerlink" href="#hessian" title="Permalink to this heading">#</a></h2>
<p>The Hessian is defined by</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^{2} l}{\partial \theta^{2}} = \left( \frac{\partial l}{\partial x} \otimes I_{p} \right) \frac{\partial^{2} x}{\partial \theta^{2}} + \frac{\partial x}{\partial \theta}^{\top}\frac{\partial^{2} l}{\partial x^{2}}\frac{\partial x}{\partial \theta}\]</div>
<p>where <span class="math notranslate nohighlight">\(\otimes\)</span> is the Kronecker product. Note that <span class="math notranslate nohighlight">\(\nabla_{\theta} x\)</span>
is the sensitivity and the second order sensitivities can be found again
via the forward method, which involve another set of ode’s, namely the
forward-forward sensitivities</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial t}\left(\frac{\partial^{2} x}{\partial \theta^{2}}\right) = \left( \frac{\partial f}{\partial x} \otimes I_{p} \right) \frac{\partial^{2} x}{\partial \theta^{2}} + \left( I_{d} \otimes \frac{\partial x}{\partial \theta}^{\top} \right) \frac{\partial^{2} f}{\partial x^{2}} \frac{\partial x}{\partial \theta}.\]</div>
<p>From before, we know that</p>
<div class="math notranslate nohighlight">
\[\frac{\partial l}{\partial x} = (-2y+2x)  \quad and \quad \frac{\partial^{2} l}{\partial x^{2}} = 2I_{d}\]</div>
<p>so our Hessian reduces to</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^{2} l}{\partial \theta^{2}} = \left( \left(-2y+2x\right) \otimes I_{p} \right) \frac{\partial^{2} x}{\partial \theta^{2}} + 2S^{\top}S,\]</div>
<p>where the second term is a good approximation to the Hessian as
mentioned previously. This is the only implementation in place so far
even though obtaining the estimate this way is relatively slow.</p>
<p>Just to demonstate how it works, lets look at the Hessian at the optimal
point. First, we obtain the optimal value</p>
<p>In [211]: import scipy.linalg,scipy.optimize</p>
<p>In [212]: boxBounds = [(0.0, 2.0), (0.0, 2.0)]</p>
<p>In [213]: res = scipy.optimize.minimize(fun=objSIR.cost,<br />
.….: jac=objSIR.sensitivity, .….: x0=theta, .….: bounds=boxBounds, .….:
method=’L-BFGS-B’)</p>
<p>Then compare again the least square estimate of the covariance matrix
against our version</p>
<p>In [211]: resLS, cov_x, infodict, mesg, ier =
scipy.optimize.leastsq(func=objSIR.residual, x0=res[‘x’],
full_output=True)</p>
<p>In [212]: HJTJ, outputHJTJ = objSIR.hessian(full_output=True)</p>
<p>In [311]: print(scipy.linalg.inv(HJTJ))</p>
<p>In [312]: print(cov_x)</p>
<p>also note the difference between the Hessian and the approximation using
the Jacobian, which is in fact what the least squares routine uses.</p>
<p>In [313]: print(scipy.linalg.inv(outputHJTJ[‘JTJ’]))</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="bvpSimple.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Solving Boundary Value Problems</p>
      </div>
    </a>
    <a class="right-next"
       href="fh.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Example: Fitz Hugh</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-setup">Model setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-sensitivity">Forward sensitivity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient">Gradient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobian">Jacobian</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjoint">Adjoint</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-comparison">Time Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian">Hessian</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Public Health England / UK Health Security Agency
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>