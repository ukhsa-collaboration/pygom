{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient estimation under square loss\n",
    "\n",
    "Assuming that we have a set of $N$ observations $y_{i}$ at specific time\n",
    "points $t_{i}$, $i = 1,\\ldots,N$, we may wish to test out a set of ode\n",
    "to see whether it fits to the data. The most natural way to test such\n",
    "*fit* is to minimize the sum of squares between our observations $y$ and\n",
    "see whether the resulting solution of the ODE and the estimated\n",
    "parameters makes sense.\n",
    "\n",
    "We assume that this estimation process will be tackled through a\n",
    "non-linear optimization point of view. However, it should be noted that\n",
    "such estimates can also be performed via MCMC or from a global\n",
    "optimization perspective. A key element in non-linear optimization is\n",
    "the gradient, which is the focus of this page.\n",
    "\n",
    "Multiple ways of obtaining the gradient have been implemented. All of\n",
    "them serve a certain purpose and may not be a viable/appropriate options\n",
    "depending on the type of ode. More generally, let $d,p$ be the number of\n",
    "states and parameters respectively. Then finite difference methods have a\n",
    "run order of $O(p+1)$ of the original ode, forward sensitivity require\n",
    "an integration of an ode of size $(d+1)p$ rather than $d$. The adjoint\n",
    "method require two run of size $d$ in principle, but actual run time is\n",
    "dependent on the number of observations.\n",
    "\n",
    "For the details of the classes and methods, please refer to {func}`mod`.\n",
    "\n",
    "## Notation\n",
    "\n",
    "We introduce the notations that will be used in the rest of the page,\n",
    "some of which may be slightly unconventional but necessary due to the\n",
    "complexity of the problem. Let $x \\in \\mathbb{R}^{d}$ and\n",
    "$\\theta \\in \\mathbb{R}^{p}$ be the states and parameters respectively.\n",
    "The term *state* or *simulation* are used interchangeably, even though\n",
    "strictly speaking a state is $x$ whereas $x(t)$ is the simulation. An\n",
    "ode is defined as\n",
    "\n",
    "$$f(x,\\theta) = \\dot{x} = \\frac{\\partial x}{\\partial t}$$\n",
    "\n",
    "and usually comes with a set of initial conditions $(x_0,t_0)$ where\n",
    "$t_0 \\le t_{i} \\forall i$. Let $g(x,\\theta)$ be a function that maps the\n",
    "set of states to the observations,\n",
    "$g : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{m}$. For compartmental\n",
    "problems, which is our focus, $\\nabla_{\\theta}g(x,\\theta)$ is usually\n",
    "zero and $\\nabla_{x}g(x,\\theta)$ is an identity function for some or all\n",
    "of the states $x$. Denote $l(x_{0},\\theta,x)$ as our cost function\n",
    "$l : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$ and $L(x_{0},\\theta,x)$ be\n",
    "the sum of $l(\\cdot)$. Both $x$ and $x_{0}$ are usually dropped for\n",
    "simplicity. We will be dealing exclusively with square loss here, which\n",
    "means that\n",
    "\n",
    "$$L(\\theta) = \\sum_{i=1}^{N} \\left\\| y_{i} - g(x(t_{i})) \\right\\|^{2} = \\mathbf{e}^{\\top} \\mathbf{e}$$\n",
    "\n",
    "where $\\mathbf{e}$ is the residual vector, with elements\n",
    "\n",
    "$$e_{i} = y_{i} - x(t_{i}).$$\n",
    "\n",
    "## Model setup\n",
    "\n",
    "Again, we demonstrate the functionalities of our classes using an SIR\n",
    "model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48daf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygom import SquareLoss, common_models\n",
    "\n",
    "import copy, time, numpy\n",
    "\n",
    "ode = common_models.SIR()\n",
    "\n",
    "paramEval = [('beta',0.5), ('gamma',1.0/3.0) ]\n",
    "\n",
    "# the initial state, normalized to zero one\n",
    "\n",
    "x0 = [1., 1.27e-6, 0.]\n",
    "\n",
    "# initial time\n",
    "\n",
    "t0 = 0\n",
    "\n",
    "ode.parameters = paramEval\n",
    "\n",
    "ode.initial_values = (x0, t0)\n",
    "\n",
    "# set the time sequence that we would like to observe\n",
    "\n",
    "t = numpy.linspace(1, 150, 100)\n",
    "\n",
    "numStep = len(t)\n",
    "\n",
    "solution = ode.integrate(t)\n",
    "\n",
    "y = solution[1::,2].copy()\n",
    "\n",
    "y += numpy.random.normal(0, 0.1, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98ce5a",
   "metadata": {},
   "source": [
    "\n",
    "Now we have set up the model along with some observations, obtaining the\n",
    "gradient only requires the end user to put the appropriate information\n",
    "it into the {class}`SquareLoss`. Given the initial guess $\\theta$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159bb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0.2, 0.2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14885bb",
   "metadata": {},
   "source": [
    "\n",
    "We initialize the {class}`SquareLoss` as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d85b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "objSIR = SquareLoss(theta, ode, x0, t0, t, y, 'R')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b21f6a",
   "metadata": {},
   "source": [
    "\n",
    "where the we also have to specify the state our observations are from.\n",
    "Now, we demonstrate the different methods in obtaining the gradient and\n",
    "mathematics behind it.\n",
    "\n",
    "## Forward sensitivity\n",
    "\n",
    "The forward sensitivity equations are derived by differentiating the\n",
    "states implicitly, which yields\n",
    "\n",
    "$$\\frac{d\\dot{x}}{d\\theta} = \\frac{\\partial f}{\\partial x}\\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta}.$$\n",
    "\n",
    "So finding the sensitivies $\\frac{dx}{d\\theta}$ require another\n",
    "integration of a $p$ coupled ODE of $d$ dimension, each with the same\n",
    "Jacobian as the original ode. This integration is performed along with\n",
    "the original ODE because of possible non-linearity.\n",
    "\n",
    "A direct call to the method {meth}`sensitivity`\n",
    "computes the gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradSens = objSIR.sensitivity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25fc403",
   "metadata": {},
   "source": [
    "\n",
    "whereas {meth}`jac` will allow the user to obtain the Jacobian (of the\n",
    "objective function) and the residuals, the information required to get\n",
    "the gradient as we see next.\n",
    "\n",
    "#TODO additional reading for Jacobian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85611719",
   "metadata": {},
   "outputs": [],
   "source": [
    "objJac, output = objSIR.jac(full_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3d25b",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient\n",
    "\n",
    "Just the sensitivities alone are not enough to obtain the gradient, but\n",
    "we are $90\\%$ there. Differentiating the loss function\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{dL}{d\\theta} &= \\nabla_{\\theta} \\sum_{i=1}^{N}\\frac{dl}{dg} \\\\\n",
    "                   &= \\sum_{i=1}^{N} \\frac{\\partial l}{\\partial x}\\frac{dx}{d\\theta} + \\frac{\\partial l}{\\partial \\theta} \\\\\n",
    "                   &= \\sum_{i=1}^{N} \\frac{\\partial l}{\\partial g}\\frac{\\partial g}{\\partial x}\\frac{dx}{d\\theta} + \\frac{\\partial l}{\\partial g}\\frac{\\partial g}{\\partial \\theta}\n",
    "\\end{aligned}$$\n",
    "\n",
    "via chain rule. When $\\frac{\\partial g}{\\partial \\theta} = 0$, the total\n",
    "gradient simplifies to\n",
    "\n",
    "$$\\frac{dL}{d\\theta} = \\sum_{i=1}^{N} \\frac{\\partial l}{\\partial g}\\frac{\\partial g}{\\partial x}\\frac{dx}{d\\theta}$$\n",
    "\n",
    "The time indicies are dropped but all the terms above\n",
    "are evaluated only at the observed time points. More concretely, this\n",
    "means that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l(x(j),\\theta)}{\\partial g} = \\left\\{ \\begin{array}{ll} -2(y_{i} - x(j)) & , \\; j = t_{i} \\\\ 0 & \\; \\text{otherwise} \\end{array} \\right.\n",
    "\\end{aligned}$$\n",
    "\n",
    "When $g(\\cdot)$ is an identity function (which is assumed to be the case\n",
    "in {class}`SquareLoss`)\n",
    "\n",
    "$$\\frac{\\partial g(x(t_{i}),\\theta)}{\\partial x} = I_{d}$$\n",
    "\n",
    "then the gradient simplifies even further as it is simply\n",
    "\n",
    "$$\\frac{dL}{d\\theta} = -2\\mathbf{e}^{\\top}\\mathbf{S}$$\n",
    "\n",
    "where $\\mathbf{e}$ is the vector of residuals and\n",
    "$\\mathbf{S} = \\left[\\mathbf{s}_{1},\\mathbf{s}_{2},\\ldots,\\mathbf{s}_{n}\\right]$\n",
    "with elements\n",
    "\n",
    "$$\\mathbf{s}_{i} = \\frac{dx}{d\\theta}(t_{i}),$$\n",
    "\n",
    "the solution of the forward sensitivies at time $t_{i}$, obtained from\n",
    "solving the coupled ode as mentioned previously.\n",
    "\n",
    "## Jacobian\n",
    "\n",
    "Now note how the gradient simplifies to $-2\\mathbf{e}^{\\top}\\mathbf{S}$.\n",
    "Recall that a standard result in non-linear programming states that the\n",
    "gradient of a sum of sqaures objective function $L(\\theta,y,x)$ is\n",
    "\n",
    "$$\\nabla_{\\theta} L(\\theta,y,x) = -2(\\mathbf{J}^{T} \\left[\\mathbf{y} - \\mathbf{f}(x,\\boldsymbol{\\theta}) \\right] )^{\\top}$$\n",
    "\n",
    "with $f(x,\\theta)$ our non-linear function and $J$ our Jacobian with\n",
    "elements\n",
    "\n",
    "$$J_{i} = \\frac{\\partial f(x_{i},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}.$$\n",
    "\n",
    "This is exactly what we have seen previously, substituting in reveals\n",
    "that $J = \\mathbf{S}$. Hence, the Jacobian is (a necessary) by-product\n",
    "when we wish to obtain the gradient. This is how we\n",
    "proceed in {meth}`sensitivity` where it makes\n",
    "an internal call to {func}`jac` to obtain the Jacobian\n",
    "first. This allows the user to have more options when choosing which\n",
    "type of algorithms to use, i.e.Â Gauss-Newton or Levenberg-Marquardt.\n",
    "\n",
    "#TODO ref for algorithms\n",
    "\n",
    "To check that the output is in fact the same we can calculate the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "objJac.transpose().dot(-2\\*output\\['resid'\\]) - gradSens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37355764",
   "metadata": {},
   "source": [
    "\n",
    "## Adjoint\n",
    "\n",
    "When the number of parameters increases, the number of sensitivies also\n",
    "increases. The time required scales directly with the number of\n",
    "parameters. We describe another method which does not depend on the\n",
    "number of parameters, but rather, the number of states and observations.\n",
    "\n",
    "The full derivations will not be shown here, \n",
    "\n",
    "#TODO reference\n",
    "\n",
    "but we aim to provide\n",
    "enough information to work out the steps performed in the our code. We can\n",
    "write our optimization problem as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "min_{\\theta} \\quad & \\int_{t_{0}}^{T} l(x_{0},\\theta,x(t)) dt \\\\\n",
    "s.t. \\quad & \\dot{x} = f(x,\\theta)\n",
    "\\end{aligned}$$\n",
    "\n",
    "which is identical to the original problem but in a continuous setting.\n",
    "Now write the constrained problem in the Lagrangian form\n",
    "\n",
    "$$min_{\\theta} \\; L(\\theta) + \\int_{t_{0}}^{T} \\lambda^{\\top}(\\dot{x} - f(x,\\theta))$$\n",
    "\n",
    "with Lagrangian multiplier $\\lambda \\ge 0$. After some algebraic\n",
    "manipulation, it can be shown that the total derivative of the\n",
    "Lagrangian function is\n",
    "\n",
    "$$\\frac{dL}{d\\theta} = \\int_{t_{0}}^{T} \\left(\\frac{\\partial l}{\\partial \\theta} - \\lambda^{\\top}\\frac{\\partial f}{\\partial \\theta} \\right) dt.$$\n",
    "\n",
    "Using previously defined loss functions (the identity), the first term\n",
    "is zero and evaluating $\\frac{\\partial f}{\\partial \\theta}$ is trivial.\n",
    "What remains is the calculation of $\\lambda(t)$ for\n",
    "$t \\in \\left[t_{0},T\\right]$.\n",
    "\n",
    "Although this still seem to be ill-posed problem when Looking at the\n",
    "Lagrangian function, one can actually obtain the *adjoint equation*,\n",
    "after certain assumptions and\n",
    "\n",
    "$$\\frac{d\\lambda^{\\top}}{dt} = \\frac{\\partial l}{\\partial x} - \\lambda^{\\top}\\frac{\\partial f}{\\partial \\theta}.$$\n",
    "\n",
    "which is again an integration. An unfortunate situation arises here for\n",
    "non-linear systems because we use the minus Jacobian in the adjoint\n",
    "equation. So if the eigenvalues of the Jacobian indicate that our\n",
    "original ODE is stable, such as -1, the minus eigenvalues (now 1)\n",
    "implies that the adjoint equation is not stable. Therefore, one must\n",
    "integrate backward in time to solve the adjoint equation and it cannot\n",
    "be solved simultaneously as the ODE, unlike the forward sensitivity\n",
    "equations.\n",
    "\n",
    "Given a non-linear ODE, we must store information about the states\n",
    "between $t_{0}$ and $T$ in order to perform the integration. There are\n",
    "two options, both require storing many evaluated $x(j)$ within the\n",
    "interval $\\left[t_{0},T\\right]$. Unfortunately, only one is available;\n",
    "interpolation over all states and integrate using the interpolating\n",
    "functions. The alternative of using observed $x(j)'s$ at fixed points is\n",
    "not competitive because we are unable to use fortran routines for the\n",
    "integration\n",
    "\n",
    "The method of choice here to perform the adjoint calculation is to run a\n",
    "forward integration, then perform an interpolation using splines with\n",
    "explicit knots at the observed time points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "odeSIRAdjoint, outputAdjoint = objSIR.adjoint(full_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28857c",
   "metadata": {},
   "source": [
    "\n",
    "This is because evaluating the Jacobian may be expensive and Runge-kutta\n",
    "method suffers as the complexity increases. In non-linear model such as\n",
    "those found in epidemiology, each element of the Jacobian may be the\n",
    "result of a complicated equation where linear step method will shine as\n",
    "it makes as little function evaluation as possible. Note that\n",
    "derivations in the literature, the initial condition when evaluating the\n",
    "adjoint equation is $\\lambda(T)=0$. But in our code we used\n",
    "$\\lambda(T) = -2(y(T)-x(T))$. Recall that we have observation $y(T)$ and\n",
    "simulation $x(T)$, so that the adjoint equation evaluated at time $T$\n",
    "\n",
    "$$\\frac{\\partial \\lambda^{\\top}}{\\partial t} \\Big|_{T} = -2(y-f(x,\\theta))\\Big|_{T}  - \\lambda(T)\\frac{\\partial f}{\\partial \\theta}\\Big|_{T}$$\n",
    "\n",
    "with the second term equal to zero. Integration under step size $h$\n",
    "implies that\n",
    "$\\lambda(T) \\approx \\lim_{h \\to 0} \\lambda(T-h) = -2(y(T)-x(T))$.\n",
    "\n",
    "## Time Comparison\n",
    "\n",
    "A simple time comparison between the different methods reveals that the\n",
    "forward sensitivity method dominates the others by a wide margin. It\n",
    "will be tempting to conclude that it is the best and should be the\n",
    "default at all times but that is not true, due to the complexity of each\n",
    "method mentioned previously. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04636d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradSens = objSIR.sensitivity()\n",
    "\n",
    "%timeit odeSIRAdjoint,outputAdjoint = objSIR.adjoint(full_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c000f6",
   "metadata": {},
   "source": [
    "```{note} \n",
    "We leave it to the user to find out the best method for their specific problem.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47256d1b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Hessian\n",
    "\n",
    "The Hessian is defined by\n",
    "\n",
    "$$\\frac{\\partial^{2} l}{\\partial \\theta^{2}} = \\left( \\frac{\\partial l}{\\partial x} \\otimes I_{p} \\right) \\frac{\\partial^{2} x}{\\partial \\theta^{2}} + \\frac{\\partial x}{\\partial \\theta}^{\\top}\\frac{\\partial^{2} l}{\\partial x^{2}}\\frac{\\partial x}{\\partial \\theta}$$\n",
    "\n",
    "where $\\otimes$ is the Kronecker product. Note that $\\nabla_{\\theta} x$\n",
    "is the sensitivity and the second order sensitivities can be found again\n",
    "via the forward method, which involve another set of ode's, namely the\n",
    "forward-forward sensitivities\n",
    "\n",
    "$$\\frac{\\partial}{\\partial t}\\left(\\frac{\\partial^{2} x}{\\partial \\theta^{2}}\\right) = \\left( \\frac{\\partial f}{\\partial x} \\otimes I_{p} \\right) \\frac{\\partial^{2} x}{\\partial \\theta^{2}} + \\left( I_{d} \\otimes \\frac{\\partial x}{\\partial \\theta}^{\\top} \\right) \\frac{\\partial^{2} f}{\\partial x^{2}} \\frac{\\partial x}{\\partial \\theta}.$$\n",
    "\n",
    "From before, we know that\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial x} = (-2y+2x)  \\quad and \\quad \\frac{\\partial^{2} l}{\\partial x^{2}} = 2I_{d}$$\n",
    "\n",
    "so our Hessian reduces to\n",
    "\n",
    "$$\\frac{\\partial^{2} l}{\\partial \\theta^{2}} = \\left( \\left(-2y+2x\\right) \\otimes I_{p} \\right) \\frac{\\partial^{2} x}{\\partial \\theta^{2}} + 2S^{\\top}S,$$\n",
    "\n",
    "where the second term is a good approximation to the Hessian as\n",
    "mentioned previously. This is the only implementation in place so far\n",
    "even though obtaining the estimate this way is relatively slow.\n",
    "\n",
    "Just to demonstrate how it works, lets look at the Hessian at the optimal\n",
    "point. First, we obtain the optimal value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678be374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg,scipy.optimize\n",
    "\n",
    "boxBounds = [(0.0, 2.0), (0.0, 2.0)]\n",
    "\n",
    "res = scipy.optimize.minimize(fun=objSIR.cost, jac=objSIR.sensitivity, x0=theta, bounds=boxBounds, method='L-BFGS-B')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c87afe",
   "metadata": {},
   "source": [
    "Then compare again the least square estimate of the covariance matrix\n",
    "against our version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87442b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "resLS, cov_x, infodict, mesg, ier = scipy.optimize.leastsq(func=objSIR.residual, x0=res['x'], full_output=True)\n",
    "\n",
    "HJTJ, outputHJTJ = objSIR.hessian(full_output=True)\n",
    "\n",
    "print(scipy.linalg.inv(HJTJ))\n",
    "\n",
    "print(cov_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef4147",
   "metadata": {},
   "source": [
    "\n",
    "also noting the difference between the Hessian and the approximation using\n",
    "the Jacobian, which is what the least squares routine uses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(scipy.linalg.inv(outputHJTJ['JTJ']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('sphinx-doc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dc1e323c80fe09539c74ad5c5a7c7d8d9ff99e04f7b3dbd3680daf878629d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
